{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM3EIC1RBmx9IPzRtvA171c"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Install required packages\n","!pip install transformers gradio torch --quiet\n","\n","# Import libraries\n","import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","import gradio as gr\n","\n","# Load a dialogue model\n","model_name = \"microsoft/DialoGPT-medium\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForCausalLM.from_pretrained(model_name)\n","\n","# Initialize chat history\n","chat_history_ids = None\n","\n","# Chatbot function\n","def chatbot(user_input):\n","    global chat_history_ids\n","\n","    # Encode user input\n","    new_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt')\n","\n","    # Concatenate with chat history\n","    if chat_history_ids is None:\n","        bot_input_ids = new_input_ids\n","    else:\n","        bot_input_ids = torch.cat([chat_history_ids, new_input_ids], dim=-1)\n","\n","    # Generate response\n","    chat_history_ids = model.generate(\n","        bot_input_ids,\n","        max_length=1000,\n","        pad_token_id=tokenizer.eos_token_id,\n","        eos_token_id=tokenizer.eos_token_id,\n","        top_p=0.9,\n","        top_k=50,\n","        temperature=0.7\n","    )\n","\n","    # Decode bot response\n","    bot_response = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n","\n","    return bot_response\n","\n","# Create Gradio interface\n","iface = gr.Interface(\n","    fn=chatbot,\n","    inputs=gr.Textbox(lines=2, placeholder=\"Type your message here...\"),\n","    outputs=gr.Textbox(label=\"Chatbot Response\"),\n","    title=\"ðŸ¤– AI Chatbot\",\n","    description=\"Type a message and the chatbot will respond like a real conversation. Type 'bye' to exit.\"\n",")\n","\n","# Launch chatbot\n","iface.launch()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":646},"id":"GJZbZK7B126q","executionInfo":{"status":"ok","timestamp":1756539083664,"user_tz":-360,"elapsed":25578,"user":{"displayName":"Nazrana Nahreen","userId":"16330717224888754153"}},"outputId":"3175b800-de93-4721-e1f2-afda6a971e57"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://50015f91c26ae2d213.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://50015f91c26ae2d213.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":7}]}]}